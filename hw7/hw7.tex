\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{bm}
\usepackage{enumerate,listings,graphicx,epstopdf,siunitx}
\usepackage{color}
\graphicspath{~/Documents/school/fall16/stat586/hw6}
\setcounter{secnumdepth}{0}

\sloppy
\definecolor{lightgray}{gray}{0.5}
 
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\normD}[3]{\frac{1}{\sqrt{2\pi #1^2}}exp\left(\frac{-( #2 - #3)^2}{2 #1^2}\right)} 
\newcommand{\cProb}[2]{P(#1|#2)}
\newcommand{\infInt}{\int_{-\infty}^{\infty}}

\begin{document}
\title{Homework Set 7}
\author{Taylor Bodin}
\maketitle

\section{Problem 6.1}
\subsection{a.}
There are 8 different realizations: \\
\\
\begin{tabular}{ c | c | c | c | c | c | c | c }
  0 & 0 & 0 & 0 & 1 & 1 & 1 & 1 \\ 
  0 & 0 & 1 & 1 & 0 & 0 & 1 & 1 \\ 
  0 & 1 & 0 & 1 & 0 & 1 & 0 & 1
\end{tabular}

\subsection{b.}
Yes. The individual coin flips are independent of one another.

\section{Problem 6.3}
The length of the arc of half of the unit circle is $\pi$.
\begin{align*}
f_S(s) &= \frac{1}{\pi}ds \\
ds &= \sqrt{1+\left(\frac{dx}{dy}\right)^2}dy \\
f_Y(y) &= \frac{1}{\pi}\sqrt{1+\left(\frac{dx}{dy}\right)^2}dy \\
&= \frac{2}{\pi}\sqrt{1+\left(\frac{-y}{\sqrt{1-y^2}}\right)^2}dy \\
1 &= \int_0^1 \frac{2}{\pi}\sqrt{1+\left(\frac{-y}{\sqrt{1-y^2}}\right)^2}dy
\end{align*}
\section{Problem 6.5}
\subsection{Given}
\begin{align*}
  P_{N_1}(n_1) &= \frac{1}{3} \\
  P_{N_2|N_1}(n_2|n_1) &= \frac{1}{n_1} \\
  P_{N_3|N_2}(n_3|n_2) &= \frac{1}{n_2} \\
\end{align*}
\subsection{a.}
\[P_{N_1,N_2}(n_1,n_2) = P_{N_2|N_1}(n_2|n_1)P_{N_1}(n1) = \frac{1}{3n_1}\]

\subsection{b.}
\begin{align*}
  P_{N_1,N_2,N_3}(n_1,n_2,n_3) &= P_{N_3|N_2,N_1}(n_3|n_2,n_1)P_{N_2|N_1}(n_2|n_1)P_{N_1}(n_1) \\
  &= P_{N_3|N_2,N_1}(n_3|n_2)P_{N_2|N_1}(n_2|n_1)P_{N_1}(n_1) \\
  &= \frac{1}{3n_1n_2}
\end{align*}

\subsection{c.}
The answer above demonstrates that $P_{N_2}$ is independent of $P_{N_3}$ therfore:
\begin{align*}
  P_{N_2} = \frac{1}{n_1} \\
  P_{N_3} = \frac{1}{n_2}
\end{align*}

\subsection{d.}
\[P_{N_1,N_2,N_3}(n_1 \ne 1, n_2 \ne 1, n_3 \ne 1) = \frac{2(n_1-1)(n_2-1)}{3n_1n_2}\]
\section{Problem 6.7}
\subsection{a.}
Let $I_n(z) = \iiint \dots \int_{\sum_{i=1}^n x_i \leq z} dx_1 dx_2 \dots dx_n$
\begin{align*}
I_1(z) &= \int_{0}^z du = z \\
I_2(z) &= \int_{0}^z zdu = \frac{z^2}{2} \\
I_3(z) &= \int_{0}^z \frac{z^2}{2}du = \frac{z^3}{3} \\
\vdots \\
I_n(z) &= \frac{1}{n!}z^n
\end{align*}
Therefore, $c=n!$

\subsection{b.}
\[\frac{N!}{(N-M)!}\left( 1- \sum_{j=1}^m X_j \right)^{N-1}\]

\subsection{c.}
They are identically distributed but not independent. 

\section{Problem 6.9}
\subsection{a.}
\begin{tabular}{|c|c|c|c|c|c|}
  \hline
  (1,1) & (1,2) & (1,3) & (1,4) & (1,5) & (1,6) \\
  (2,1) & (2,2) & (2,3) & (2,4) & (2,5) & (2,6) \\
  (3,1) & (3,2) & (3,3) & (3,4) & (3,5) & (3,6) \\
  (4,1) & (4,2) & (4,3) & (4,4) & (4,5) & (4,6) \\
  (5,1) & (5,2) & (5,3) & (5,4) & (5,5) & (5,6) \\
  (6,1) & (6,2) & (6,3) & (6,4) & (6,5) & (6,6) \\
  \hline
\end{tabular}

\subsection{b.}
They have equal probability of $\frac{1}{36}$.

\subsection{c.}
Since each roll is independent $ E[(X,Y)] = (E[X],E[Y]) = (3.5,3.5)$

\subsection{d.}
\begin{align*}
  Cov(X,Y) &= Cov(Y,X) = 0 \\
  Cov(X,X) &= Cov(Y,Y) = \frac{1}{6}\sum_1^6 x^2 - 3.5^2 = \frac{35}{12}  \\
  \bm{C_{XY}} &= 
    \begin{bmatrix}
    Cov(X,X) & Cov(X,Y) \\
    Cov(Y,X) & Cov(Y,Y)
    \end{bmatrix} \\
  \bm{C_{XY}} &= 
    \begin{bmatrix}
    \frac{35}{12} & 0 \\
    0 & \frac{35}{12}
    \end{bmatrix}
\end{align*}

\section{Problem 6.11}
\subsection{a.}
\[
  E[\bm{X}] = \bm{\mu} =  \left[\frac{1}{2} \ \frac{1}{2} \ \frac{1}{2} \dots \right]
\]
\subsection{b.}
\begin{align*}
  \bm{R_{XX}} &= E[\bm{XX^T}] \\
  &= \begin{bmatrix}
    \frac{1}{3} & \frac{1}{4} & \dots  & \frac{1}{4} \\
    \frac{1}{4} & \frac{1}{3} & \dots  & \frac{1}{4} \\
    \vdots & \vdots & \ddots & \vdots & \\
    \frac{1}{4} & \frac{1}{4} & \dots  & \frac{1}{3}
  \end{bmatrix}
\end{align*}

\subsection{c.}
\begin{align*}
  \bm{C_{XX}} &= \bm{R_{XX} - \bm{\mu\mu^T}} \\
  &= \begin{bmatrix}
    \frac{1}{12} & 0 & \dots  & 0 \\
    0 & \frac{1}{12} & \dots  & 0 \\
    \vdots & \vdots & \ddots & \vdots & \\
    0 & 0 & \dots  & \frac{1}{12}
  \end{bmatrix}
\end{align*}

\section{6.13}
\begin{align*}
  \bm{\mu\mu^T} &= -1\left( \bm{C_XX - R_XX} \right) \\
  &= \begin{bmatrix}
    1 & 2 & -1 \\
    2 & 4 & -2 \\
   -1 & -2 & 1 
 \end{bmatrix} & \textrm{then by inspection} \\
 &= \begin{bmatrix}
   1 \\
   2 \\
   -1
 \end{bmatrix}
\end{align*}

\section{Problem 6.15}
\begin{align*}
  det(\bm{C_{XX}}) &= 23 \\
  \bm{C_{XX}^{-1}} &= \frac{1}{23} 
  \begin{bmatrix}
    11 & 4 & -5 \\
    2 & 7 & -3 \\
    3 & -1 & 7
  \end{bmatrix} \\
  f_{\bm{X}}(\bm{x}) &= \frac{1}{(2\pi)^3 23} exp\left( \frac{11x^2}{23} + \frac{6xy}{23}-\frac{2xz}{23} +
  \frac{7y^2 - 4yz + 7z^2}{23}\right)
\end{align*}

\section{Problem 6.17}

\section{Problem 7.1}
\subsection{a.}
\begin{align*}
  E[\hat\mu] &= E\left[ \frac{1}{10}\sum_1^{10} X_i \right] \\
  &= \frac{1}{10}\sum_1^{10}E[X_i] \\
  &= \frac{1}{10}\sum_1^{10}5 \\
  &= 5
\end{align*}
\subsection{b.}
\[Var(\hat\mu) = \frac{\sigma_X^2}{n} = \frac{1}{10} \]
\subsection{c.}
Because it's an unbiased estimator:
\[E[\hat s] = \sigma_X^2 = 1\]

\section{Problem 7.3}
\begin{align*}
  E[\hat\mu] &= 0 \\
  Var[\hat\mu] &= \frac{\sigma^2}{N} = \frac{.01}{100} = 10^{-4} \\
  \hat\mu \sim N(0,10^{-4})
\end{align*}

\section{Problem 7.5}
\begin{align*}
  \hat\mu &= \frac{1}{n}\sum X_m \\
  \hat\sigma^2 &= \frac{1}{n}\sum (X_m-\hat\mu)^2 \\
  &= \frac{1}{n}\sum \left(X_m^2-2X_m\hat\mu + \hat\mu^2\right) \\
  &= \frac{1}{n}\sum X_m^2 -\frac{2}{n^2} \sum X_m \sum X_m + \frac{1}{n^2}\left(\sum X_m\right)^2 \\
  &= E\left[\frac{1}{n}\sum X_m^2 -\frac{1}{n^2} \left(\sum X_m\right)^2\right] \\
  &= \mu^2 + \sigma^2 - \frac{1}{n^2}\left( n\sigma^2 + n^2\mu^2 \right) \\
  &= \frac{n-1}{n}\sigma^2
\end{align*}

\section{Problem 7.7}
\begin{align*}
  Y_m &= X_m - \hat\mu \\
  Cov(\hat\mu,Y_m) &= R_{\hat\mu,Y_m} - \mu_{\hat\mu}\mu_{Y_m} \\
  &= E\left[ (\hat\mu - \mu)(Y_{m} - \hat\mu) \right] \\
  &= E[\hat\mu X_m] - E[\hat\mu^2] - \mu E[X_m] + \mu E[\hat\mu] \\
  &= \frac{\sigma_X^2}{n} - \mu^2 - (\frac{\sigma_X^2}{n} + \mu^2) - \mu^2 + \mu^2 \\
  &= 0 & & \therefore \textrm{Sample mean is independent of sample variance.}
\end{align*}

\section*{Problem 6.45}

\begin{par}
Taylor Bodin
\end{par} \vspace{1em}

\subsection*{Contents}

\begin{itemize}
\setlength{\itemsep}{-1ex}
   \item Data entry
   \item Positive Definite
   \item Symmetric
   \item Answer
\end{itemize}


\subsection*{Data entry}

\begin{verbatim}
ca = [3 -2 1;2 6 0;-1 0 2];
cb = [3 -2 3;-2 6 0;3 0 2];
cc = [3 -2 1;-2 6 0;1 0 2];
cd = [1 -1/2 1/4 -1/8;-1/2 1 -1/2 1/4;1/4 -1/2 1 -1/2;-1/8 1/4 -1/2 1];
ce = [11 -3 7 5;-3 11 5 7;7 5 11 -3;5 7 -3 11];
cf = [5 1 3 -1;1 5 -1 3;3 -1 5 1;-1 3 1 5];

mats = {ca cb cd ce cf};
\end{verbatim}


\subsection*{Positive Definite}

\begin{verbatim}
pd = zeros(1,5);
for i=1:5
pd(i) = all(eig(mats{i})>0);
end
\end{verbatim}


\subsection*{Symmetric}

\begin{verbatim}
sy = zeros(1,5);
for i=1:5
    sy(i) = isequal(mats{i},mats{i}');
end
\end{verbatim}


\subsection*{Answer}

\begin{verbatim}
valid_cov = sy.*pd;
disp(valid_cov)
\end{verbatim}

        \color{lightgray} \begin{verbatim}     0     0     1     0     1

\end{verbatim} \color{black}
    
\end{document}
